{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2e1195-af99-4a63-b1c7-b6d8c99848ff",
   "metadata": {},
   "source": [
    "### Q1 Running Mage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c9674-9cc4-4c75-9b2d-8369b69ca519",
   "metadata": {},
   "source": [
    "Answer 1: v0.9.72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6be3de-9d75-494d-9e88-c8788f4836ec",
   "metadata": {},
   "source": [
    "### Q2 Reading the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58476b65-8148-40fe-9a3a-578d191689bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2073e5-daa4-4044-bafb-c6f18e21fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    line = line.strip()\n",
    "    line = line.strip('\\uFEFF')\n",
    "    return line\n",
    "\n",
    "# Read the files\n",
    "def read_faq(file_id):\n",
    "    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with io.BytesIO(response.content) as f_in:\n",
    "        doc = docx.Document(f_in)\n",
    "\n",
    "    questions = []\n",
    "\n",
    "    question_heading_style = 'heading 2'\n",
    "    section_heading_style = 'heading 1'\n",
    "    \n",
    "    heading_id = ''\n",
    "    section_title = ''\n",
    "    question_title = ''\n",
    "    answer_text_so_far = ''\n",
    "     \n",
    "    for p in doc.paragraphs:\n",
    "        style = p.style.name.lower()\n",
    "        p_text = clean_line(p.text)\n",
    "    \n",
    "        if len(p_text) == 0:\n",
    "            continue\n",
    "    \n",
    "        if style == section_heading_style:\n",
    "            section_title = p_text\n",
    "            continue\n",
    "    \n",
    "        if style == question_heading_style:\n",
    "            answer_text_so_far = answer_text_so_far.strip()\n",
    "            if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "                questions.append({\n",
    "                    'text': answer_text_so_far,\n",
    "                    'section': section_title,\n",
    "                    'question': question_title,\n",
    "                })\n",
    "                answer_text_so_far = ''\n",
    "    \n",
    "            question_title = p_text\n",
    "            continue\n",
    "        \n",
    "        answer_text_so_far += '\\n' + p_text\n",
    "    \n",
    "    answer_text_so_far = answer_text_so_far.strip()\n",
    "    if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "        questions.append({\n",
    "            'text': answer_text_so_far,\n",
    "            'section': section_title,\n",
    "            'question': question_title,\n",
    "        })\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8217878-c0ce-4c29-b126-437e468835f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the documents into a dictionary\n",
    "def ingest():\n",
    "    faq_documents = {\n",
    "        'llm-zoomcamp': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n",
    "    }\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for course, file_id in faq_documents.items():\n",
    "        #print(course)\n",
    "        course_documents = read_faq(file_id)\n",
    "        documents.append({'course': course, 'documents': course_documents})\n",
    "    \n",
    "    print(\"Answer 2:\", len(documents))\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5099f4-4787-4c12-b194-4b3b5e0f23a6",
   "metadata": {},
   "source": [
    "### Mage code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f2d91-c69a-4a62-a08d-624fd35edf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final code for the custom block, do not execute here\n",
    "\n",
    "if 'data_loader' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_loader\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "import io\n",
    "import requests\n",
    "import docx\n",
    "    \n",
    "@data_loader\n",
    "def load_data(*args, **kwargs):\n",
    "    faq_documents = {\n",
    "        'llm-zoomcamp': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n",
    "    }\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for course, file_id in faq_documents.items():\n",
    "        print(course)\n",
    "        course_documents = read_faq(file_id)\n",
    "        documents.append({'course': course, 'documents': course_documents})\n",
    "    \n",
    "    print(\"Number of documents: \", len(documents))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def clean_line(line):\n",
    "    line = line.strip()\n",
    "    line = line.strip('\\uFEFF')\n",
    "    return line\n",
    "\n",
    "def read_faq(file_id):\n",
    "    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with io.BytesIO(response.content) as f_in:\n",
    "        doc = docx.Document(f_in)\n",
    "\n",
    "    questions = []\n",
    "\n",
    "    question_heading_style = 'heading 2'\n",
    "    section_heading_style = 'heading 1'\n",
    "    \n",
    "    heading_id = ''\n",
    "    section_title = ''\n",
    "    question_title = ''\n",
    "    answer_text_so_far = ''\n",
    "     \n",
    "    for p in doc.paragraphs:\n",
    "        style = p.style.name.lower()\n",
    "        p_text = clean_line(p.text)\n",
    "    \n",
    "        if len(p_text) == 0:\n",
    "            continue\n",
    "    \n",
    "        if style == section_heading_style:\n",
    "            section_title = p_text\n",
    "            continue\n",
    "    \n",
    "        if style == question_heading_style:\n",
    "            answer_text_so_far = answer_text_so_far.strip()\n",
    "            if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "                questions.append({\n",
    "                    'text': answer_text_so_far,\n",
    "                    'section': section_title,\n",
    "                    'question': question_title,\n",
    "                })\n",
    "                answer_text_so_far = ''\n",
    "    \n",
    "            question_title = p_text\n",
    "            continue\n",
    "        \n",
    "        answer_text_so_far += '\\n' + p_text\n",
    "    \n",
    "    answer_text_so_far = answer_text_so_far.strip()\n",
    "    if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "        questions.append({\n",
    "            'text': answer_text_so_far,\n",
    "            'section': section_title,\n",
    "            'question': question_title,\n",
    "        })\n",
    "\n",
    "    return questions\n",
    "    \n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615ed41-565a-4fc4-8571-aab986ad35e8",
   "metadata": {},
   "source": [
    "### Q3 Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783a1f0-6195-47cb-aa33-e2da2f683e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# Chunking the documents\n",
    "def chunking():\n",
    "    data = ingest()\n",
    "    #print(type(data))\n",
    "    #print(data)\n",
    "    documents = []\n",
    "    \n",
    "    for course_dict in data:  # Loop through each dictionary in the list\n",
    "        #print(\"Course dict:\", course_dict)\n",
    "        for doc in course_dict['documents']:  # Then loop through the documents in each dictionary\n",
    "            doc['course'] = course_dict['course']\n",
    "            doc['document_id'] = generate_document_id(doc)\n",
    "            documents.append(doc)\n",
    "    \n",
    "    print(\"Answer 3:\", len(documents), \"chunks\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Generating the document ids\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n",
    "\n",
    "chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13559512-5ab6-425b-806a-029fb158248b",
   "metadata": {},
   "source": [
    "### Mage code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b89a6c-c798-4bee-a601-7272159db80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'transformer' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import transformer\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "import hashlib\n",
    "\n",
    "@transformer\n",
    "def transform(data, *args, **kwargs):\n",
    "    print(type(data))\n",
    "    print(\"Printing data:\", data)\n",
    "    documents = []\n",
    "\n",
    "    for doc in data['documents']:\n",
    "        doc['course'] = data['course']\n",
    "        # previously we used just \"id\" for document ID\n",
    "        doc['document_id'] = generate_document_id(doc)\n",
    "        documents.append(doc)\n",
    "\n",
    "    print(\"Number of chunks:\", len(documents))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n",
    "    \n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8f0a6-64d1-44a2-a447-8cc1052f0c79",
   "metadata": {},
   "source": [
    "### Q4 Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4772c-637f-4e1a-a2e9-bce6964c1e8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "\n",
    "def elasticsearch_export(\n",
    "    documents: List[Dict[str, Union[Dict, List[int], np.ndarray, str]]], \n",
    "    connection_string='https://elasticsearch:9200/',\n",
    "    index_name_prefix='documents',\n",
    "    number_of_shards=1,\n",
    "    number_of_replicas=0,\n",
    "    vector_column_name='embedding',\n",
    "    dimensions=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Exports document data to an Elasticsearch database.\n",
    "    \"\"\"\n",
    "\n",
    "    # Adjusting index name\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n",
    "    index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "    print(\"index name:\", index_name)\n",
    "\n",
    "    # Connection to Elasticsearch\n",
    "    es_client = Elasticsearch(connection_string)\n",
    "    print(f'Connecting to Elasticsearch at {connection_string}')\n",
    "\n",
    "    # Determine dimensions if not provided\n",
    "    if dimensions is None and len(documents) > 0:\n",
    "        document = documents[0]\n",
    "        dimensions = len(document.get(vector_column_name) or [])\n",
    "\n",
    "    # Index settings\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": number_of_shards,\n",
    "            \"number_of_replicas\": number_of_replicas\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"section\": {\"type\": \"text\"},\n",
    "                \"question\": {\"type\": \"text\"},\n",
    "                \"course\": {\"type\": \"keyword\"},\n",
    "                \"document_id\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create index if it doesn't exist\n",
    "    if not es_client.indices.exists(index=index_name):\n",
    "        es_client.indices.create(index=index_name, body=index_settings)\n",
    "        print('Index created with properties:', index_settings)\n",
    "        print('Embedding dimensions:', dimensions)\n",
    "\n",
    "    # Index documents\n",
    "    print(f'Indexing {len(documents)} documents to Elasticsearch index {index_name}')\n",
    "    for document in documents:\n",
    "        print(f'Indexing document {document[\"document_id\"]}')\n",
    "        es_client.index(index=index_name, document=document)\n",
    "\n",
    "    print(\"Indexing completed.\")\n",
    "\n",
    "# Example usage (make sure to define your `documents` list)\n",
    "documents = chunking()\n",
    "elasticsearch_export(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac54dc-8ed1-468a-9879-70ace73d15f0",
   "metadata": {},
   "source": [
    "### Mage code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b77b1-f7fa-4b01-9155-69f6c2d514ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final code\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "from mage_ai.data_preparation.variable_manager import set_global_variable\n",
    "\n",
    "if 'data_exporter' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_exporter\n",
    "\n",
    "@data_exporter\n",
    "def elasticsearch(\n",
    "    documents: List[Dict[str, Union[Dict, List[int], np.ndarray, str]]], *args, **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Exports document data to an Elasticsearch database.\n",
    "    \"\"\"\n",
    "\n",
    "    connection_string = kwargs.get('connection_string', 'http://localhost:9200/')\n",
    "\n",
    "    # Adjusting index name\n",
    "    index_name_prefix = kwargs.get('index_name', 'documents')\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n",
    "    index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "    print(\"index name:\", index_name)\n",
    "\n",
    "    # Setting global variable\n",
    "    set_global_variable('resplendent_radiance', 'index_name', index_name)\n",
    "\n",
    "    number_of_shards = kwargs.get('number_of_shards', 1)\n",
    "    number_of_replicas = kwargs.get('number_of_replicas', 0)\n",
    "    vector_column_name = kwargs.get('vector_column_name', 'embedding')\n",
    "\n",
    "    dimensions = kwargs.get('dimensions')\n",
    "    if dimensions is None and len(documents) > 0:\n",
    "        document = documents[0]\n",
    "        dimensions = len(document.get(vector_column_name) or [])\n",
    "\n",
    "    es_client = Elasticsearch(connection_string)\n",
    "\n",
    "    print(f'Connecting to Elasticsearch at {connection_string}')\n",
    "\n",
    "    index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": number_of_shards,\n",
    "        \"number_of_replicas\": number_of_replicas\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"document_id\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "    if not es_client.indices.exists(index=index_name):\n",
    "        es_client.indices.create(index=index_name)\n",
    "        print('Index created with properties:', index_settings)\n",
    "        print('Embedding dimensions:', dimensions)\n",
    "\n",
    "    print(f'Indexing {len(documents)} documents to Elasticsearch index {index_name}')\n",
    "    for document in documents:\n",
    "        print(f'Indexing document {document[\"document_id\"]}')\n",
    "\n",
    "        es_client.index(index=index_name, document=document)\n",
    "\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf0eef-7e25-4e52-91ab-301d384e74fa",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08621a-24af-491c-a3af-73d4f996a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "index name: documents_20240820_2506\n",
    "index name: documents_20240820_2506\n",
    "Connecting to Elasticsearch at http://elasticsearch:9200\n",
    "Connecting to Elasticsearch at http://elasticsearch:9200\n",
    "Index created with properties: {'settings': {'number_of_shards': 1, 'number_of_replicas': 0}, 'mappings': {'properties': {'text': {'type': 'text'}, 'section': {'type': 'text'}, 'question': {'type': 'text'}, 'course': {'type': 'keyword'}, 'document_id': {'type': 'keyword'}}}}\n",
    "Embedding dimensions: 0\n",
    "Indexing 86 documents to Elasticsearch index documents_20240820_2506\n",
    "Indexing document 97872393\n",
    "Index created with properties: {'settings': {'number_of_shards': 1, 'number_of_replicas': 0}, 'mappings': {'properties': {'text': {'type': 'text'}, 'section': {'type': 'text'}, 'question': {'type': 'text'}, 'course': {'type': 'keyword'}, 'document_id': {'type': 'keyword'}}}}\n",
    "Embedding dimensions: 0\n",
    "Indexing 86 documents to Elasticsearch index documents_20240820_2506\n",
    "Indexing document 97872393\n",
    "Indexing document a57f9581\n",
    "Indexing document fb81c6ff\n",
    "Indexing document bf024675\n",
    "Indexing document e0d2caf7\n",
    "Indexing document a57f9581\n",
    "Indexing document fb81c6ff\n",
    "Indexing document bf024675\n",
    "Indexing document e0d2caf7\n",
    "Indexing document 7bd989aa\n",
    "Indexing document 1c96a1fb\n",
    "Indexing document 01cb301e\n",
    "Indexing document ca68d283\n",
    "Indexing document 6fc3236a\n",
    "Indexing document 6d61aae2\n",
    "Indexing document cbe66cfe\n",
    "Indexing document 7bd989aa\n",
    "Indexing document 1c96a1fb\n",
    "Indexing document 01cb301e\n",
    "Indexing document ca68d283\n",
    "Indexing document 6fc3236a\n",
    "Indexing document 6d61aae2\n",
    "Indexing document cbe66cfe\n",
    "Indexing document a5301a1f\n",
    "Indexing document 9816f1ae\n",
    "Indexing document 98c1bc60\n",
    "Indexing document befeedef\n",
    "Indexing document baea0a66\n",
    "Indexing document a976d6e7\n",
    "Indexing document a5301a1f\n",
    "Indexing document 9816f1ae\n",
    "Indexing document 98c1bc60\n",
    "Indexing document befeedef\n",
    "Indexing document baea0a66\n",
    "Indexing document a976d6e7\n",
    "Indexing document 18a32cec\n",
    "Indexing document 764f2789\n",
    "Indexing document baae926f\n",
    "Indexing document 190fc999\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 1a9b8b53\n",
    "Indexing document a310259a\n",
    "Indexing document 5a995cf3\n",
    "Indexing document 18a32cec\n",
    "Indexing document 764f2789\n",
    "Indexing document baae926f\n",
    "Indexing document 190fc999\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 1a9b8b53\n",
    "Indexing document a310259a\n",
    "Indexing document 5a995cf3\n",
    "Indexing document 67da0fb5\n",
    "Indexing document a1ec19a2\n",
    "Indexing document d710981f\n",
    "Indexing document 19811ec0\n",
    "Indexing document aa8f7017\n",
    "Indexing document 48312d67\n",
    "Indexing document 1c862647\n",
    "Indexing document 67da0fb5\n",
    "Indexing document a1ec19a2\n",
    "Indexing document d710981f\n",
    "Indexing document 19811ec0\n",
    "Indexing document aa8f7017\n",
    "Indexing document 48312d67\n",
    "Indexing document 1c862647\n",
    "Indexing document fd874951\n",
    "Indexing document 0536ca0b\n",
    "Indexing document 8ac0422d\n",
    "Indexing document 6ef32048\n",
    "Indexing document 3ffb9e62\n",
    "Indexing document 8efc052a\n",
    "Indexing document 7b87b859\n",
    "Indexing document fd874951\n",
    "Indexing document 0536ca0b\n",
    "Indexing document 8ac0422d\n",
    "Indexing document 6ef32048\n",
    "Indexing document 3ffb9e62\n",
    "Indexing document 8efc052a\n",
    "Indexing document 7b87b859\n",
    "Indexing document 5734b048\n",
    "Indexing document 1804f538\n",
    "Indexing document f8f7469d\n",
    "Indexing document 4b95ba51\n",
    "Indexing document 12f1a26a\n",
    "Indexing document aace1f4a\n",
    "Indexing document db816465\n",
    "Indexing document eb00a0c9\n",
    "Indexing document 5734b048\n",
    "Indexing document 1804f538\n",
    "Indexing document f8f7469d\n",
    "Indexing document 4b95ba51\n",
    "Indexing document 12f1a26a\n",
    "Indexing document aace1f4a\n",
    "Indexing document db816465\n",
    "Indexing document eb00a0c9\n",
    "Indexing document 1eb85e18\n",
    "Indexing document 54dd72ba\n",
    "Indexing document 464b4d9c\n",
    "Indexing document 2806a1c1\n",
    "Indexing document 9068bbd5\n",
    "Indexing document ee355823\n",
    "Indexing document 0a101a81\n",
    "Indexing document 84ef78df\n",
    "Indexing document 1eb85e18\n",
    "Indexing document 54dd72ba\n",
    "Indexing document 464b4d9c\n",
    "Indexing document 2806a1c1\n",
    "Indexing document 9068bbd5\n",
    "Indexing document ee355823\n",
    "Indexing document 0a101a81\n",
    "Indexing document 84ef78df\n",
    "Indexing document a1419bf6\n",
    "Indexing document 5f8fd79d\n",
    "Indexing document 0deabb27\n",
    "Indexing document a2dca2e2\n",
    "Indexing document a262c532\n",
    "Indexing document 8912e711\n",
    "Indexing document 005ecede\n",
    "Indexing document fe48ad62\n",
    "Indexing document a1419bf6\n",
    "Indexing document 5f8fd79d\n",
    "Indexing document 0deabb27\n",
    "Indexing document a2dca2e2\n",
    "Indexing document a262c532\n",
    "Indexing document 8912e711\n",
    "Indexing document 005ecede\n",
    "Indexing document fe48ad62\n",
    "Indexing document c13c26c8\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 258a03fe\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 794ed89c\n",
    "Indexing document e9107390\n",
    "Indexing document 43b399a8\n",
    "Indexing document c13c26c8\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 258a03fe\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 794ed89c\n",
    "Indexing document e9107390\n",
    "Indexing document 43b399a8\n",
    "Indexing document 534f8148\n",
    "Indexing document 79f67e08\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 1fc5e366\n",
    "Indexing document 6cf805ca\n",
    "Indexing document e18124d4\n",
    "Indexing document a705279d\n",
    "Indexing document f5f83001\n",
    "Indexing document 534f8148\n",
    "Indexing document 79f67e08\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 1fc5e366\n",
    "Indexing document 6cf805ca\n",
    "Indexing document e18124d4\n",
    "Indexing document a705279d\n",
    "Indexing document f5f83001\n",
    "Indexing document db752798\n",
    "Indexing document e2433e15\n",
    "Indexing document 99ab2f5d\n",
    "Indexing document f250bb18\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document fa136280\n",
    "{'text': 'Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.', 'section': 'General course-related questions', 'question': 'I missed the first homework - can I still get a certificate?', 'course': 'llm-zoomcamp', 'document_id': 'fa136280'}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
