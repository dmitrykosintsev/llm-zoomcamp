{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa2e1195-af99-4a63-b1c7-b6d8c99848ff",
   "metadata": {},
   "source": [
    "### Q1 Running Mage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220c9674-9cc4-4c75-9b2d-8369b69ca519",
   "metadata": {},
   "source": [
    "Answer 1: v0.9.72"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6be3de-9d75-494d-9e88-c8788f4836ec",
   "metadata": {},
   "source": [
    "### Q2 Reading the documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58476b65-8148-40fe-9a3a-578d191689bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('attempt to write a readonly database')).History will not be written to the database.\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "import requests\n",
    "import docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed2073e5-daa4-4044-bafb-c6f18e21fe6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_line(line):\n",
    "    line = line.strip()\n",
    "    line = line.strip('\\uFEFF')\n",
    "    return line\n",
    "\n",
    "# Read the files\n",
    "def read_faq(file_id):\n",
    "    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with io.BytesIO(response.content) as f_in:\n",
    "        doc = docx.Document(f_in)\n",
    "\n",
    "    questions = []\n",
    "\n",
    "    question_heading_style = 'heading 2'\n",
    "    section_heading_style = 'heading 1'\n",
    "    \n",
    "    heading_id = ''\n",
    "    section_title = ''\n",
    "    question_title = ''\n",
    "    answer_text_so_far = ''\n",
    "     \n",
    "    for p in doc.paragraphs:\n",
    "        style = p.style.name.lower()\n",
    "        p_text = clean_line(p.text)\n",
    "    \n",
    "        if len(p_text) == 0:\n",
    "            continue\n",
    "    \n",
    "        if style == section_heading_style:\n",
    "            section_title = p_text\n",
    "            continue\n",
    "    \n",
    "        if style == question_heading_style:\n",
    "            answer_text_so_far = answer_text_so_far.strip()\n",
    "            if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "                questions.append({\n",
    "                    'text': answer_text_so_far,\n",
    "                    'section': section_title,\n",
    "                    'question': question_title,\n",
    "                })\n",
    "                answer_text_so_far = ''\n",
    "    \n",
    "            question_title = p_text\n",
    "            continue\n",
    "        \n",
    "        answer_text_so_far += '\\n' + p_text\n",
    "    \n",
    "    answer_text_so_far = answer_text_so_far.strip()\n",
    "    if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "        questions.append({\n",
    "            'text': answer_text_so_far,\n",
    "            'section': section_title,\n",
    "            'question': question_title,\n",
    "        })\n",
    "\n",
    "    return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8217878-c0ce-4c29-b126-437e468835f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put the documents into a dictionary\n",
    "def ingest():\n",
    "    faq_documents = {\n",
    "        'llm-zoomcamp': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n",
    "    }\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for course, file_id in faq_documents.items():\n",
    "        #print(course)\n",
    "        course_documents = read_faq(file_id)\n",
    "        documents.append({'course': course, 'documents': course_documents})\n",
    "    \n",
    "    print(\"Answer 2:\", len(documents))\n",
    "\n",
    "    return documents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd5099f4-4787-4c12-b194-4b3b5e0f23a6",
   "metadata": {},
   "source": [
    "### Mage code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f2d91-c69a-4a62-a08d-624fd35edf24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final code for the custom block, do not execute here\n",
    "\n",
    "if 'data_loader' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_loader\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "\n",
    "import io\n",
    "import requests\n",
    "import docx\n",
    "    \n",
    "@data_loader\n",
    "def load_data(*args, **kwargs):\n",
    "    faq_documents = {\n",
    "        'llm-zoomcamp': '1qZjwHkvP0lXHiE4zdbWyUXSVfmVGzougDD6N37bat3E',\n",
    "    }\n",
    "\n",
    "    documents = []\n",
    "\n",
    "    for course, file_id in faq_documents.items():\n",
    "        print(course)\n",
    "        course_documents = read_faq(file_id)\n",
    "        documents.append({'course': course, 'documents': course_documents})\n",
    "    \n",
    "    print(\"Number of documents: \", len(documents))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def clean_line(line):\n",
    "    line = line.strip()\n",
    "    line = line.strip('\\uFEFF')\n",
    "    return line\n",
    "\n",
    "def read_faq(file_id):\n",
    "    url = f'https://docs.google.com/document/d/{file_id}/export?format=docx'\n",
    "    \n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    with io.BytesIO(response.content) as f_in:\n",
    "        doc = docx.Document(f_in)\n",
    "\n",
    "    questions = []\n",
    "\n",
    "    question_heading_style = 'heading 2'\n",
    "    section_heading_style = 'heading 1'\n",
    "    \n",
    "    heading_id = ''\n",
    "    section_title = ''\n",
    "    question_title = ''\n",
    "    answer_text_so_far = ''\n",
    "     \n",
    "    for p in doc.paragraphs:\n",
    "        style = p.style.name.lower()\n",
    "        p_text = clean_line(p.text)\n",
    "    \n",
    "        if len(p_text) == 0:\n",
    "            continue\n",
    "    \n",
    "        if style == section_heading_style:\n",
    "            section_title = p_text\n",
    "            continue\n",
    "    \n",
    "        if style == question_heading_style:\n",
    "            answer_text_so_far = answer_text_so_far.strip()\n",
    "            if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "                questions.append({\n",
    "                    'text': answer_text_so_far,\n",
    "                    'section': section_title,\n",
    "                    'question': question_title,\n",
    "                })\n",
    "                answer_text_so_far = ''\n",
    "    \n",
    "            question_title = p_text\n",
    "            continue\n",
    "        \n",
    "        answer_text_so_far += '\\n' + p_text\n",
    "    \n",
    "    answer_text_so_far = answer_text_so_far.strip()\n",
    "    if answer_text_so_far != '' and section_title != '' and question_title != '':\n",
    "        questions.append({\n",
    "            'text': answer_text_so_far,\n",
    "            'section': section_title,\n",
    "            'question': question_title,\n",
    "        })\n",
    "\n",
    "    return questions\n",
    "    \n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c615ed41-565a-4fc4-8571-aab986ad35e8",
   "metadata": {},
   "source": [
    "### Q3 Chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5783a1f0-6195-47cb-aa33-e2da2f683e61",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import hashlib\n",
    "\n",
    "# Chunking the documents\n",
    "def chunking():\n",
    "    data = ingest()\n",
    "    #print(type(data))\n",
    "    #print(data)\n",
    "    documents = []\n",
    "    \n",
    "    for course_dict in data:  # Loop through each dictionary in the list\n",
    "        #print(\"Course dict:\", course_dict)\n",
    "        for doc in course_dict['documents']:  # Then loop through the documents in each dictionary\n",
    "            doc['course'] = course_dict['course']\n",
    "            doc['document_id'] = generate_document_id(doc)\n",
    "            documents.append(doc)\n",
    "    \n",
    "    print(\"Answer 3:\", len(documents), \"chunks\")\n",
    "    \n",
    "    return documents\n",
    "\n",
    "# Generating the document ids\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n",
    "\n",
    "chunking()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13559512-5ab6-425b-806a-029fb158248b",
   "metadata": {},
   "source": [
    "### Mage code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b89a6c-c798-4bee-a601-7272159db80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'transformer' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import transformer\n",
    "if 'test' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import test\n",
    "import hashlib\n",
    "\n",
    "@transformer\n",
    "def transform(data, *args, **kwargs):\n",
    "    #print(type(data))\n",
    "    #print(\"Printing data:\", data)\n",
    "    documents = []\n",
    "\n",
    "    for doc in data['documents']:\n",
    "        doc['course'] = data['course']\n",
    "        # previously we used just \"id\" for document ID\n",
    "        doc['document_id'] = generate_document_id(doc)\n",
    "        documents.append(doc)\n",
    "\n",
    "    print(\"Number of chunks:\", len(documents))\n",
    "\n",
    "    return documents\n",
    "\n",
    "def generate_document_id(doc):\n",
    "    combined = f\"{doc['course']}-{doc['question']}-{doc['text'][:10]}\"\n",
    "    hash_object = hashlib.md5(combined.encode())\n",
    "    hash_hex = hash_object.hexdigest()\n",
    "    document_id = hash_hex[:8]\n",
    "    return document_id\n",
    "    \n",
    "@test\n",
    "def test_output(output, *args) -> None:\n",
    "    \"\"\"\n",
    "    Template code for testing the output of the block.\n",
    "    \"\"\"\n",
    "    assert output is not None, 'The output is undefined'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f8f0a6-64d1-44a2-a447-8cc1052f0c79",
   "metadata": {},
   "source": [
    "### Q4 Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02f4772c-637f-4e1a-a2e9-bce6964c1e8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from typing import Dict, List, Union\n",
    "\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "\n",
    "def elasticsearch_export(\n",
    "    documents: List[Dict[str, Union[Dict, List[int], np.ndarray, str]]], \n",
    "    connection_string='https://elasticsearch:9200/',\n",
    "    index_name_prefix='documents',\n",
    "    number_of_shards=1,\n",
    "    number_of_replicas=0,\n",
    "    vector_column_name='embedding',\n",
    "    dimensions=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Exports document data to an Elasticsearch database.\n",
    "    \"\"\"\n",
    "\n",
    "    # Adjusting index name\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n",
    "    index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "    print(\"index name:\", index_name)\n",
    "\n",
    "    # Connection to Elasticsearch\n",
    "    es_client = Elasticsearch(connection_string)\n",
    "    print(f'Connecting to Elasticsearch at {connection_string}')\n",
    "\n",
    "    # Determine dimensions if not provided\n",
    "    if dimensions is None and len(documents) > 0:\n",
    "        document = documents[0]\n",
    "        dimensions = len(document.get(vector_column_name) or [])\n",
    "\n",
    "    # Index settings\n",
    "    index_settings = {\n",
    "        \"settings\": {\n",
    "            \"number_of_shards\": number_of_shards,\n",
    "            \"number_of_replicas\": number_of_replicas\n",
    "        },\n",
    "        \"mappings\": {\n",
    "            \"properties\": {\n",
    "                \"text\": {\"type\": \"text\"},\n",
    "                \"section\": {\"type\": \"text\"},\n",
    "                \"question\": {\"type\": \"text\"},\n",
    "                \"course\": {\"type\": \"keyword\"},\n",
    "                \"document_id\": {\"type\": \"keyword\"}\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # Create index if it doesn't exist\n",
    "    if not es_client.indices.exists(index=index_name):\n",
    "        es_client.indices.create(index=index_name, body=index_settings)\n",
    "        print('Index created with properties:', index_settings)\n",
    "        print('Embedding dimensions:', dimensions)\n",
    "\n",
    "    # Index documents\n",
    "    print(f'Indexing {len(documents)} documents to Elasticsearch index {index_name}')\n",
    "    for document in documents:\n",
    "        print(f'Indexing document {document[\"document_id\"]}')\n",
    "        es_client.index(index=index_name, document=document)\n",
    "\n",
    "    print(\"Indexing completed.\")\n",
    "\n",
    "# Example usage (make sure to define your `documents` list)\n",
    "documents = chunking()\n",
    "elasticsearch_export(documents)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abac54dc-8ed1-468a-9879-70ace73d15f0",
   "metadata": {},
   "source": [
    "### Mage code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239b77b1-f7fa-4b01-9155-69f6c2d514ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final code\n",
    "from typing import Dict, List, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from elasticsearch import Elasticsearch\n",
    "from datetime import datetime\n",
    "from mage_ai.data_preparation.variable_manager import set_global_variable\n",
    "\n",
    "if 'data_exporter' not in globals():\n",
    "    from mage_ai.data_preparation.decorators import data_exporter\n",
    "\n",
    "@data_exporter\n",
    "def elasticsearch(\n",
    "    documents: List[Dict[str, Union[Dict, List[int], np.ndarray, str]]], *args, **kwargs,\n",
    "):\n",
    "    \"\"\"\n",
    "    Exports document data to an Elasticsearch database.\n",
    "    \"\"\"\n",
    "\n",
    "    connection_string = kwargs.get('connection_string', 'http://localhost:9200/')\n",
    "\n",
    "    # Adjusting index name\n",
    "    index_name_prefix = kwargs.get('index_name', 'documents')\n",
    "    current_time = datetime.now().strftime(\"%Y%m%d_%M%S\")\n",
    "    index_name = f\"{index_name_prefix}_{current_time}\"\n",
    "    print(\"index name:\", index_name)\n",
    "\n",
    "    # Setting global variable\n",
    "    set_global_variable('resplendent_radiance', 'index_name', index_name)\n",
    "\n",
    "    number_of_shards = kwargs.get('number_of_shards', 1)\n",
    "    number_of_replicas = kwargs.get('number_of_replicas', 0)\n",
    "    vector_column_name = kwargs.get('vector_column_name', 'embedding')\n",
    "\n",
    "    dimensions = kwargs.get('dimensions')\n",
    "    if dimensions is None and len(documents) > 0:\n",
    "        document = documents[0]\n",
    "        dimensions = len(document.get(vector_column_name) or [])\n",
    "\n",
    "    es_client = Elasticsearch(connection_string)\n",
    "\n",
    "    print(f'Connecting to Elasticsearch at {connection_string}')\n",
    "\n",
    "    index_settings = {\n",
    "    \"settings\": {\n",
    "        \"number_of_shards\": number_of_shards,\n",
    "        \"number_of_replicas\": number_of_replicas\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"properties\": {\n",
    "            \"text\": {\"type\": \"text\"},\n",
    "            \"section\": {\"type\": \"text\"},\n",
    "            \"question\": {\"type\": \"text\"},\n",
    "            \"course\": {\"type\": \"keyword\"},\n",
    "            \"document_id\": {\"type\": \"keyword\"}\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "    if not es_client.indices.exists(index=index_name):\n",
    "        es_client.indices.create(index=index_name)\n",
    "        print('Index created with properties:', index_settings)\n",
    "        print('Embedding dimensions:', dimensions)\n",
    "\n",
    "    print(f'Indexing {len(documents)} documents to Elasticsearch index {index_name}')\n",
    "    for document in documents:\n",
    "        print(f'Indexing document {document[\"document_id\"]}')\n",
    "\n",
    "        es_client.index(index=index_name, document=document)\n",
    "\n",
    "    print(document)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75bf0eef-7e25-4e52-91ab-301d384e74fa",
   "metadata": {},
   "source": [
    "Output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c08621a-24af-491c-a3af-73d4f996a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "index name: documents_20240820_2506\n",
    "index name: documents_20240820_2506\n",
    "Connecting to Elasticsearch at http://elasticsearch:9200\n",
    "Connecting to Elasticsearch at http://elasticsearch:9200\n",
    "Index created with properties: {'settings': {'number_of_shards': 1, 'number_of_replicas': 0}, 'mappings': {'properties': {'text': {'type': 'text'}, 'section': {'type': 'text'}, 'question': {'type': 'text'}, 'course': {'type': 'keyword'}, 'document_id': {'type': 'keyword'}}}}\n",
    "Embedding dimensions: 0\n",
    "Indexing 86 documents to Elasticsearch index documents_20240820_2506\n",
    "Indexing document 97872393\n",
    "Index created with properties: {'settings': {'number_of_shards': 1, 'number_of_replicas': 0}, 'mappings': {'properties': {'text': {'type': 'text'}, 'section': {'type': 'text'}, 'question': {'type': 'text'}, 'course': {'type': 'keyword'}, 'document_id': {'type': 'keyword'}}}}\n",
    "Embedding dimensions: 0\n",
    "Indexing 86 documents to Elasticsearch index documents_20240820_2506\n",
    "Indexing document 97872393\n",
    "Indexing document a57f9581\n",
    "Indexing document fb81c6ff\n",
    "Indexing document bf024675\n",
    "Indexing document e0d2caf7\n",
    "Indexing document a57f9581\n",
    "Indexing document fb81c6ff\n",
    "Indexing document bf024675\n",
    "Indexing document e0d2caf7\n",
    "Indexing document 7bd989aa\n",
    "Indexing document 1c96a1fb\n",
    "Indexing document 01cb301e\n",
    "Indexing document ca68d283\n",
    "Indexing document 6fc3236a\n",
    "Indexing document 6d61aae2\n",
    "Indexing document cbe66cfe\n",
    "Indexing document 7bd989aa\n",
    "Indexing document 1c96a1fb\n",
    "Indexing document 01cb301e\n",
    "Indexing document ca68d283\n",
    "Indexing document 6fc3236a\n",
    "Indexing document 6d61aae2\n",
    "Indexing document cbe66cfe\n",
    "Indexing document a5301a1f\n",
    "Indexing document 9816f1ae\n",
    "Indexing document 98c1bc60\n",
    "Indexing document befeedef\n",
    "Indexing document baea0a66\n",
    "Indexing document a976d6e7\n",
    "Indexing document a5301a1f\n",
    "Indexing document 9816f1ae\n",
    "Indexing document 98c1bc60\n",
    "Indexing document befeedef\n",
    "Indexing document baea0a66\n",
    "Indexing document a976d6e7\n",
    "Indexing document 18a32cec\n",
    "Indexing document 764f2789\n",
    "Indexing document baae926f\n",
    "Indexing document 190fc999\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 1a9b8b53\n",
    "Indexing document a310259a\n",
    "Indexing document 5a995cf3\n",
    "Indexing document 18a32cec\n",
    "Indexing document 764f2789\n",
    "Indexing document baae926f\n",
    "Indexing document 190fc999\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 1a9b8b53\n",
    "Indexing document a310259a\n",
    "Indexing document 5a995cf3\n",
    "Indexing document 67da0fb5\n",
    "Indexing document a1ec19a2\n",
    "Indexing document d710981f\n",
    "Indexing document 19811ec0\n",
    "Indexing document aa8f7017\n",
    "Indexing document 48312d67\n",
    "Indexing document 1c862647\n",
    "Indexing document 67da0fb5\n",
    "Indexing document a1ec19a2\n",
    "Indexing document d710981f\n",
    "Indexing document 19811ec0\n",
    "Indexing document aa8f7017\n",
    "Indexing document 48312d67\n",
    "Indexing document 1c862647\n",
    "Indexing document fd874951\n",
    "Indexing document 0536ca0b\n",
    "Indexing document 8ac0422d\n",
    "Indexing document 6ef32048\n",
    "Indexing document 3ffb9e62\n",
    "Indexing document 8efc052a\n",
    "Indexing document 7b87b859\n",
    "Indexing document fd874951\n",
    "Indexing document 0536ca0b\n",
    "Indexing document 8ac0422d\n",
    "Indexing document 6ef32048\n",
    "Indexing document 3ffb9e62\n",
    "Indexing document 8efc052a\n",
    "Indexing document 7b87b859\n",
    "Indexing document 5734b048\n",
    "Indexing document 1804f538\n",
    "Indexing document f8f7469d\n",
    "Indexing document 4b95ba51\n",
    "Indexing document 12f1a26a\n",
    "Indexing document aace1f4a\n",
    "Indexing document db816465\n",
    "Indexing document eb00a0c9\n",
    "Indexing document 5734b048\n",
    "Indexing document 1804f538\n",
    "Indexing document f8f7469d\n",
    "Indexing document 4b95ba51\n",
    "Indexing document 12f1a26a\n",
    "Indexing document aace1f4a\n",
    "Indexing document db816465\n",
    "Indexing document eb00a0c9\n",
    "Indexing document 1eb85e18\n",
    "Indexing document 54dd72ba\n",
    "Indexing document 464b4d9c\n",
    "Indexing document 2806a1c1\n",
    "Indexing document 9068bbd5\n",
    "Indexing document ee355823\n",
    "Indexing document 0a101a81\n",
    "Indexing document 84ef78df\n",
    "Indexing document 1eb85e18\n",
    "Indexing document 54dd72ba\n",
    "Indexing document 464b4d9c\n",
    "Indexing document 2806a1c1\n",
    "Indexing document 9068bbd5\n",
    "Indexing document ee355823\n",
    "Indexing document 0a101a81\n",
    "Indexing document 84ef78df\n",
    "Indexing document a1419bf6\n",
    "Indexing document 5f8fd79d\n",
    "Indexing document 0deabb27\n",
    "Indexing document a2dca2e2\n",
    "Indexing document a262c532\n",
    "Indexing document 8912e711\n",
    "Indexing document 005ecede\n",
    "Indexing document fe48ad62\n",
    "Indexing document a1419bf6\n",
    "Indexing document 5f8fd79d\n",
    "Indexing document 0deabb27\n",
    "Indexing document a2dca2e2\n",
    "Indexing document a262c532\n",
    "Indexing document 8912e711\n",
    "Indexing document 005ecede\n",
    "Indexing document fe48ad62\n",
    "Indexing document c13c26c8\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 258a03fe\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 794ed89c\n",
    "Indexing document e9107390\n",
    "Indexing document 43b399a8\n",
    "Indexing document c13c26c8\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 258a03fe\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 794ed89c\n",
    "Indexing document e9107390\n",
    "Indexing document 43b399a8\n",
    "Indexing document 534f8148\n",
    "Indexing document 79f67e08\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 1fc5e366\n",
    "Indexing document 6cf805ca\n",
    "Indexing document e18124d4\n",
    "Indexing document a705279d\n",
    "Indexing document f5f83001\n",
    "Indexing document 534f8148\n",
    "Indexing document 79f67e08\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document 1fc5e366\n",
    "Indexing document 6cf805ca\n",
    "Indexing document e18124d4\n",
    "Indexing document a705279d\n",
    "Indexing document f5f83001\n",
    "Indexing document db752798\n",
    "Indexing document e2433e15\n",
    "Indexing document 99ab2f5d\n",
    "Indexing document f250bb18\n",
    "Indexing document d8c4c7bb\n",
    "Indexing document fa136280\n",
    "{'text': 'Yes, you need to pass the Capstone project to get the certificate. Homework is not mandatory, though it is recommended for reinforcing concepts, and the points awarded count towards your rank on the leaderboard.', 'section': 'General course-related questions', 'question': 'I missed the first homework - can I still get a certificate?', 'course': 'llm-zoomcamp', 'document_id': 'fa136280'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fd85ef-d387-456d-93c8-b81625af9211",
   "metadata": {},
   "source": [
    "### Q5 Testing the retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c3be0a34-6c26-4f28-8d10-c75b4bdda19e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: -7jShJEBde6cDBFCgGfA\n",
      "Source: {'text': 'Summer 2025 (via Alexey).', 'section': 'General course-related questions', 'question': 'When will the course be offered next?', 'course': 'llm-zoomcamp', 'document_id': 'bf024675'}\n",
      "Document ID: RrjShJEBde6cDBFCg2iE\n",
      "Source: {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort.\\nWe don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\\nYou can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.\", 'section': 'General course-related questions', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'course': 'llm-zoomcamp', 'document_id': 'a705279d'}\n",
      "Document ID: DbjShJEBde6cDBFCgWhy\n",
      "Source: {'text': 'This is likely to be an error when indexing the data. First you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query.', 'section': 'Module 1: Introduction', 'question': 'Returning Empty list after filtering my query (HW Q3)', 'course': 'llm-zoomcamp', 'document_id': '190fc999'}\n",
      "Document ID: LbjShJEBde6cDBFCgmii\n",
      "Source: {'text': 'Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.', 'section': 'Module 3: X', 'question': 'What is the cosine similarity?', 'course': 'llm-zoomcamp', 'document_id': 'ee355823'}\n",
      "Document ID: RLjShJEBde6cDBFCg2hw\n",
      "Source: {'text': 'The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings', 'section': 'Workshops: dlthub', 'question': 'There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)', 'course': 'llm-zoomcamp', 'document_id': '6cf805ca'}\n",
      "Document ID: RbjShJEBde6cDBFCg2h5\n",
      "Source: {'text': 'Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")', 'section': 'Workshops: dlthub', 'question': 'There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)', 'course': 'llm-zoomcamp', 'document_id': 'e18124d4'}\n",
      "Document ID: BLjShJEBde6cDBFCgWga\n",
      "Source: {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.', 'section': 'General course-related questions', 'question': 'What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?', 'course': 'llm-zoomcamp', 'document_id': 'a5301a1f'}\n",
      "Document ID: O7jShJEBde6cDBFCg2gg\n",
      "Source: {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. Click on the Jump to your record on the leaderboard link to find your entry.\\nIf you want to see what your Display name is, click on the Edit Course Profile button.\\nFirst field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous.\\nUnless you want “Lucid Elbakyan” on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver’s license, etc. This is the name that is going to appear on your Certificate!', 'section': 'General course-related questions', 'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?', 'course': 'llm-zoomcamp', 'document_id': '258a03fe'}\n",
      "Document ID: _rjShJEBde6cDBFCgGfd\n",
      "Source: {'text': 'You may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account:', 'section': 'Module 1: Introduction', 'question': 'OpenAI: Error when running OpenAI chat.completions.create command', 'course': 'llm-zoomcamp', 'document_id': '1c96a1fb'}\n",
      "Document ID: -rjShJEBde6cDBFCgGe1\n",
      "Source: {'text': 'This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.', 'section': 'General course-related questions', 'question': 'I was working on next week’s homework/content - why does it keep changing?', 'course': 'llm-zoomcamp', 'document_id': 'fb81c6ff'}\n"
     ]
    }
   ],
   "source": [
    "# Running the retrieval process in Jupyter\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"multi_match\": {\n",
    "            \"query\": \"When is the next cohort?\",\n",
    "            \"fields\": [\"text\", \"question\", \"section\"],\n",
    "            \"type\": \"best_fields\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Assuming you have an Elasticsearch client instance\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Replace 'your_index_name' with the actual name of your index\n",
    "response = es.search(index=\"documents_20240824_3753\", body=query)\n",
    "\n",
    "# Process the response\n",
    "hits = response['hits']['hits']\n",
    "for hit in hits:\n",
    "    print(f\"Document ID: {hit['_id']}\")\n",
    "    print(f\"Source: {hit['_source']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef879ca-cb61-415b-a53a-fa4675030a83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer 5: {'text': 'Summer 2025 (via Alexey).', 'section': 'General course-related questions', 'question': 'When will the course be offered next?', 'course': 'llm-zoomcamp', 'document_id': 'bf024675'}\n"
     ]
    }
   ],
   "source": [
    "print(\"Answer 5:\", response['hits']['hits'][0]['_source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "709aa1eb-eb31-452b-8a6a-56e0cf5fdda0",
   "metadata": {},
   "source": [
    "### Q6 Reindexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "584832e7-5021-4325-aad2-7897b77313ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document ID: 9svehJEBWtOlcaJvRQYd\n",
      "Source: {'text': 'Summer 2026.', 'section': 'General course-related questions', 'question': 'When is the next cohort?', 'course': 'llm-zoomcamp', 'document_id': 'b6fa77f3'}\n",
      "Document ID: QcvehJEBWtOlcaJvSAeW\n",
      "Source: {'text': \"No, you can only get a certificate if you finish the course with a “live” cohort.\\nWe don't award certificates for the self-paced mode. The reason is you need to peer-review 3 capstone(s) after submitting your own project.\\nYou can only peer-review projects at the time the course is running; after the form is closed and the peer-review list is compiled.\", 'section': 'General course-related questions', 'question': 'Certificate - Can I follow the course in a self-paced mode and get a certificate?', 'course': 'llm-zoomcamp', 'document_id': 'a705279d'}\n",
      "Document ID: CMvehJEBWtOlcaJvRgcV\n",
      "Source: {'text': 'This is likely to be an error when indexing the data. First you need to add the index settings before adding the data to the indices, then you will be good to go applying your filters and query.', 'section': 'Module 1: Introduction', 'question': 'Returning Empty list after filtering my query (HW Q3)', 'course': 'llm-zoomcamp', 'document_id': '190fc999'}\n",
      "Document ID: KMvehJEBWtOlcaJvRweW\n",
      "Source: {'text': 'Cosine similarity is a measure used to calculate the similarity between two non-zero vectors, often used in text analysis to determine how similar two documents are based on their content. This metric computes the cosine of the angle between two vectors, which are typically word counts or TF-IDF values of the documents. The cosine similarity value ranges from -1 to 1, where 1 indicates that the vectors are identical, 0 indicates that the vectors are orthogonal (no similarity), and -1 represents completely opposite vectors.', 'section': 'Module 3: X', 'question': 'What is the cosine similarity?', 'course': 'llm-zoomcamp', 'document_id': 'ee355823'}\n",
      "Document ID: P8vehJEBWtOlcaJvSAeA\n",
      "Source: {'text': 'The error indicates that you have not changed all instances of “employee_handbook” to “homework” in your pipeline settings', 'section': 'Workshops: dlthub', 'question': 'There is an error when opening the table using dbtable = db.open_table(\"notion_pages___homework\"): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)', 'course': 'llm-zoomcamp', 'document_id': '6cf805ca'}\n",
      "Document ID: _8vehJEBWtOlcaJvRQaZ\n",
      "Source: {'text': 'The zoom link is only published to instructors/presenters/TAs.\\nStudents participate via Youtube Live and submit questions to Slido (link would be pinned in the chat when Alexey goes Live). The video URL should be posted in the announcements channel on Telegram & Slack before it begins. Also, you will see it live on the DataTalksClub YouTube Channel.\\nDon’t post your questions in chat as it would be off-screen before the instructors/moderators have a chance to answer it if the room is very active.', 'section': 'General course-related questions', 'question': 'What is the video/zoom link to the stream for the “Office Hours” or live/workshop sessions?', 'course': 'llm-zoomcamp', 'document_id': 'a5301a1f'}\n",
      "Document ID: QMvehJEBWtOlcaJvSAeL\n",
      "Source: {'text': 'Make sure you open the correct table in line 3: dbtable = db.open_table(\"notion_pages___homework\")', 'section': 'Workshops: dlthub', 'question': 'There is an error when running main(): FileNotFoundError: Table notion_pages___homework does not exist.Please first call db.create_table(notion_pages___homework, data)', 'course': 'llm-zoomcamp', 'document_id': 'e18124d4'}\n",
      "Document ID: NsvehJEBWtOlcaJvSAcg\n",
      "Source: {'text': 'When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. Click on the Jump to your record on the leaderboard link to find your entry.\\nIf you want to see what your Display name is, click on the Edit Course Profile button.\\nFirst field is your nickname/displayed-name, change it if you want to be known as your Slack username or Github username or whatever nickname of your choice, if you want to remain anonymous.\\nUnless you want “Lucid Elbakyan” on your certificate, it is mandatory that you change the second field to your official name as in your identification documents - passport, national ID card, driver’s license, etc. This is the name that is going to appear on your Certificate!', 'section': 'General course-related questions', 'question': 'Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?', 'course': 'llm-zoomcamp', 'document_id': '258a03fe'}\n",
      "Document ID: -cvehJEBWtOlcaJvRQZH\n",
      "Source: {'text': 'You may receive the following error when running the OpenAI chat.completions.create command due to insufficient credits in your OpenAI account:', 'section': 'Module 1: Introduction', 'question': 'OpenAI: Error when running OpenAI chat.completions.create command', 'course': 'llm-zoomcamp', 'document_id': '1c96a1fb'}\n",
      "Document ID: 9cvehJEBWtOlcaJvRQYP\n",
      "Source: {'text': 'This course is being offered for the first time, and things will keep changing until a given module is ready, at which point it shall be announced. Working on the material/homework in advance will be at your own risk, as the final version could be different.', 'section': 'General course-related questions', 'question': 'I was working on next week’s homework/content - why does it keep changing?', 'course': 'llm-zoomcamp', 'document_id': 'fb81c6ff'}\n",
      "\n",
      "\n",
      "Answer 6: {'text': 'Summer 2026.', 'section': 'General course-related questions', 'question': 'When is the next cohort?', 'course': 'llm-zoomcamp', 'document_id': 'b6fa77f3'}\n"
     ]
    }
   ],
   "source": [
    "# Running the retrieval process in Jupyter\n",
    "query = {\n",
    "    \"query\": {\n",
    "        \"multi_match\": {\n",
    "            \"query\": \"When is the next cohort?\",\n",
    "            \"fields\": [\"text\", \"question\", \"section\"],\n",
    "            \"type\": \"best_fields\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "from elasticsearch import Elasticsearch\n",
    "\n",
    "# Assuming you have an Elasticsearch client instance\n",
    "es = Elasticsearch(\"http://localhost:9200\")\n",
    "\n",
    "# Replace 'your_index_name' with the actual name of your index\n",
    "response = es.search(index=\"documents_20240824_5044\", body=query)\n",
    "\n",
    "# Process the response\n",
    "hits = response['hits']['hits']\n",
    "for hit in hits:\n",
    "    print(f\"Document ID: {hit['_id']}\")\n",
    "    print(f\"Source: {hit['_source']}\")\n",
    "\n",
    "print(\"\\n\\nAnswer 6:\", response['hits']['hits'][0]['_source'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290dd0fa-a1b2-442d-ab89-cc0946e916ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
